{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "60a78031-444f-40fd-9812-8e309c2575f1",
      "metadata": {
        "id": "60a78031-444f-40fd-9812-8e309c2575f1"
      },
      "source": [
        "# Import Libraries\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fad2e09d-c7d4-4248-9fe2-219f9f5d7d54",
      "metadata": {
        "id": "fad2e09d-c7d4-4248-9fe2-219f9f5d7d54"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.data import Dataset\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Device selection in TensorFlow\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "if len(physical_devices) > 0:\n",
        "    device = \"GPU\"\n",
        "    print(\"Using device: GPU\")\n",
        "else:\n",
        "    device = \"CPU\"\n",
        "    print(\"Using device: CPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10400e48-c217-4265-bd91-ab522db463e0",
      "metadata": {
        "id": "10400e48-c217-4265-bd91-ab522db463e0"
      },
      "outputs": [],
      "source": [
        "# Load MNIST from TensorFlow Datasets\n",
        "ds, ds_info = tfds.load('mnist', split=['train', 'test'], as_supervised=True, with_info=True)\n",
        "\n",
        "train_ds, test_ds = ds\n",
        "\n",
        "# Normalize images and batch datasets\n",
        "def normalize_img(image, label):\n",
        "    image = tf.cast(image, tf.float32) / 255.0  # Shape: (28, 28, 1)\n",
        "    return image, label\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "train_ds = train_ds.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_ds = train_ds.cache().shuffle(1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "test_ds = test_ds.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "test_ds = test_ds.batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Count samples\n",
        "num_train_samples = ds_info.splits['train'].num_examples\n",
        "num_test_samples = ds_info.splits['test'].num_examples\n",
        "\n",
        "print(f\"Train samples: {num_train_samples}, Test samples: {num_test_samples}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb4c27c1-bb08-4e20-be03-6a1027ce6f29",
      "metadata": {
        "id": "bb4c27c1-bb08-4e20-be03-6a1027ce6f29"
      },
      "source": [
        "# Data Explainantion\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27b321b2-9f55-45ef-8e6b-8be367ca0206",
      "metadata": {
        "id": "27b321b2-9f55-45ef-8e6b-8be367ca0206"
      },
      "outputs": [],
      "source": [
        "# Load the dataset as before\n",
        "ds, ds_info = tfds.load('mnist', split=['train', 'test'], as_supervised=True, with_info=True)\n",
        "train_ds, test_ds = ds\n",
        "\n",
        "# 1. Number of classes\n",
        "num_classes = ds_info.features['label'].num_classes\n",
        "print(f\"Number of classes in the dataset: {num_classes}\")\n",
        "\n",
        "# 2. Class Distribution and Imbalance Check\n",
        "# Extract all labels from the train dataset\n",
        "train_labels = []\n",
        "for _, label in tfds.as_numpy(train_ds):\n",
        "    train_labels.append(label)\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "class_counts = np.bincount(train_labels)\n",
        "class_labels = np.arange(num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e43a1d7c-8c36-4da2-8157-9ed1266af0f0",
      "metadata": {
        "id": "e43a1d7c-8c36-4da2-8157-9ed1266af0f0"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 5.5))\n",
        "sns.barplot(x=class_labels, y=class_counts, palette=\"viridis\", dodge=False)\n",
        "plt.title(\"Class Distribution in MNIST Training Set\")\n",
        "plt.xlabel(\"Digit Class\")\n",
        "plt.ylabel(\"Number of Samples\")\n",
        "plt.xticks(class_labels)\n",
        "for i, v in enumerate(class_counts):\n",
        "    plt.text(i, v, str(v), color='black', va=\"bottom\")\n",
        "plt.savefig(\"class_distribution.png\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "889d93f4-1643-4d6d-925d-08c223b34e43",
      "metadata": {
        "id": "889d93f4-1643-4d6d-925d-08c223b34e43"
      },
      "outputs": [],
      "source": [
        "# Load train dataset as before\n",
        "train_ds = tfds.load('mnist', split='train', as_supervised=True)\n",
        "num_classes = 10\n",
        "\n",
        "# Collect one sample image for each class\n",
        "sample_images = [None] * num_classes\n",
        "\n",
        "for image, label in tfds.as_numpy(train_ds):\n",
        "    if sample_images[label] is None:\n",
        "        sample_images[label] = image\n",
        "    if all(img is not None for img in sample_images):\n",
        "        break\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i in range(num_classes):\n",
        "    plt.subplot(2, 5, i + 1)\n",
        "    plt.imshow(np.squeeze(sample_images[i]), cmap=\"gray\")\n",
        "    plt.title(f\"Class {i}\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.suptitle(\"Sample Images from Each Class in MNIST Training Set\")\n",
        "plt.savefig(\"sample_images.png\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "848fcc58-51aa-494f-be36-8722fec027d8",
      "metadata": {
        "id": "848fcc58-51aa-494f-be36-8722fec027d8"
      },
      "source": [
        "Initial Clustering training Images\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4824048-e5eb-42f4-a96a-5e141377320c",
      "metadata": {
        "id": "e4824048-e5eb-42f4-a96a-5e141377320c"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load train images (as before)\n",
        "train_ds = tfds.load('mnist', split='train', as_supervised=True)\n",
        "\n",
        "# Collect all images (as numpy, normalized)\n",
        "raw_train_images = []\n",
        "for image, _ in tfds.as_numpy(train_ds):\n",
        "    # image shape (28, 28, 1)\n",
        "    raw_train_images.append(image.astype(np.float32) / 255.0)\n",
        "\n",
        "raw_train_images = np.stack(raw_train_images)  # shape (N, 28, 28, 1)\n",
        "raw_train_images_flat = raw_train_images.reshape(-1, 28*28)  # (N, 784)\n",
        "\n",
        "# KMeans clustering\n",
        "kmeans_init = KMeans(n_clusters=10, n_init=20, random_state=42)\n",
        "initial_kmeans_labels = kmeans_init.fit_predict(raw_train_images_flat)\n",
        "\n",
        "print(\"Initial clustering completed.\")\n",
        "\n",
        "# t-SNE visualization (subsample 1000 images for speed)\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=500)\n",
        "raw_images_2d = tsne.fit_transform(raw_train_images_flat[:1000])\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.scatterplot(x=raw_images_2d[:, 0], y=raw_images_2d[:, 1], hue=initial_kmeans_labels[:1000], palette='tab10', s=15)\n",
        "plt.title(\"Initial KMeans Clustering on Raw MNIST Training Images\")\n",
        "plt.savefig('initial_clustering_raw.png')\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "print(\"Visualization completed. Plot saved as 'initial_clustering_raw.png'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "302fc718-e954-48e1-9f97-ad810fe037ef",
      "metadata": {
        "id": "302fc718-e954-48e1-9f97-ad810fe037ef"
      },
      "source": [
        "# Define Model\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d2b4c0b-3503-4c2f-8509-92d67ee3b848",
      "metadata": {
        "id": "6d2b4c0b-3503-4c2f-8509-92d67ee3b848"
      },
      "outputs": [],
      "source": [
        "def preprocess_for_resnet(batch):\n",
        "    # batch shape: (batch_size, 28, 28, 1)\n",
        "    # Resize\n",
        "    batch = tf.image.resize(batch, [224, 224], method='bilinear')\n",
        "    # Repeat channels: from (batch_size, 224, 224, 1) to (batch_size, 224, 224, 3)\n",
        "    batch = tf.image.grayscale_to_rgb(batch)\n",
        "    return batch\n",
        "\n",
        "# Get one batch from the training dataset iterator\n",
        "images, labels = next(iter(train_ds))\n",
        "\n",
        "print(\"Original batch shape:\", images.shape)\n",
        "\n",
        "# Apply preprocessing for ResNet\n",
        "inputs = preprocess_for_resnet(images)\n",
        "print(\"After preprocess_for_resnet:\", inputs.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f5b73bb6-fc65-42cb-8227-c5e4beb946da",
      "metadata": {
        "id": "f5b73bb6-fc65-42cb-8227-c5e4beb946da"
      },
      "outputs": [],
      "source": [
        "class ResNetAutoencoder(tf.keras.Model):\n",
        "    def __init__(self, base_model=\"resnet50\", pretrained=True):\n",
        "        super().__init__()\n",
        "        # 1) Load any Keras ResNet and strip off avgpool+fc\n",
        "        if base_model == \"resnet18\":\n",
        "            raise NotImplementedError(\"ResNet18 is not available in tf.keras.applications, use resnet50 or resnet101, etc.\")\n",
        "        keras_resnet = getattr(tf.keras.applications, base_model)(\n",
        "            include_top=False,\n",
        "            weights='imagenet' if pretrained else None,\n",
        "            input_shape=(224, 224, 3)\n",
        "        )\n",
        "        self.encoder = keras_resnet\n",
        "\n",
        "        # 2) Figure out how many channels it outputs\n",
        "        dummy = tf.zeros((1, 224, 224, 3))\n",
        "        feat = self.encoder(dummy)\n",
        "        feat_dim = feat.shape[-1]      # e.g. 2048 for ResNet50\n",
        "        self._feat_dim = feat_dim\n",
        "\n",
        "        # 3) Decoder: (batch, 7, 7, feat_dim) → (batch, 28, 28, 1)\n",
        "        self.decoder = models.Sequential([\n",
        "            layers.Conv2DTranspose(feat_dim // 2, kernel_size=3, strides=2, padding=\"same\", output_padding=1), # (14, 14, feat_dim//2)\n",
        "            layers.ReLU(),\n",
        "            layers.Conv2DTranspose(feat_dim // 4, kernel_size=3, strides=2, padding=\"same\", output_padding=1), # (28, 28, feat_dim//4)\n",
        "            layers.ReLU(),\n",
        "            layers.Conv2D(feat_dim // 4, 64, kernel_size=3, padding=\"same\"),\n",
        "            layers.ReLU(),\n",
        "            layers.Conv2D(64, 1, kernel_size=3, padding=\"same\"),\n",
        "            layers.Activation('sigmoid')\n",
        "        ])\n",
        "\n",
        "    def call(self, x):\n",
        "        z = self.encoder(x)  # (batch, 7, 7, feat_dim)\n",
        "        z_flat = tf.reduce_mean(z, axis=[1, 2])  # global average pooling (batch, feat_dim)\n",
        "        recon = self.decoder(z)  # (batch, 28, 28, 1)\n",
        "        return recon, z_flat"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "857f2bc8-5cf4-499f-86dd-f6e0c3dbe1c3",
      "metadata": {
        "id": "857f2bc8-5cf4-499f-86dd-f6e0c3dbe1c3"
      },
      "source": [
        "Pre-train Autoencoder\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "325e5b31-2837-4baf-9058-90306a0e5718",
      "metadata": {
        "id": "325e5b31-2837-4baf-9058-90306a0e5718"
      },
      "outputs": [],
      "source": [
        "# Instantiate the model\n",
        "autoencoder = ResNetAutoencoder(base_model=\"ResNet50\")  # Note: use \"ResNet50\" (capitalization matches tf.keras.applications)\n",
        "\n",
        "# Build model once to initialize variables (dummy input)\n",
        "dummy_input = tf.zeros((1, 224, 224, 3))\n",
        "recon, z_flat = autoencoder(dummy_input)\n",
        "\n",
        "# Loss function (Mean Squared Error)\n",
        "loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "print(\"Encoder output channels:\", autoencoder._feat_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35c22547-c56a-4260-af1c-ec0903f65cc0",
      "metadata": {
        "id": "35c22547-c56a-4260-af1c-ec0903f65cc0"
      },
      "outputs": [],
      "source": [
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # ——— Training ———\n",
        "    running_train_loss = 0.0\n",
        "    num_train_samples = 0\n",
        "\n",
        "    for images, _ in train_ds:  # train_ds is batched\n",
        "        # Preprocess for ResNet\n",
        "        inputs = preprocess_for_resnet(images)\n",
        "        # Forward and backward pass\n",
        "        with tf.GradientTape() as tape:\n",
        "            outputs, _ = autoencoder(inputs, training=True)\n",
        "            loss = loss_fn(images, outputs)\n",
        "        grads = tape.gradient(loss, autoencoder.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, autoencoder.trainable_variables))\n",
        "\n",
        "        batch_size = tf.shape(images)[0].numpy()\n",
        "        running_train_loss += loss.numpy() * batch_size\n",
        "        num_train_samples += batch_size\n",
        "\n",
        "    avg_train_loss = running_train_loss / num_train_samples\n",
        "\n",
        "    # ——— Validation ———\n",
        "    running_val_loss = 0.0\n",
        "    num_val_samples = 0\n",
        "\n",
        "    for images, _ in test_ds:  # test_ds is batched\n",
        "        inputs = preprocess_for_resnet(images)\n",
        "        outputs, _ = autoencoder(inputs, training=False)\n",
        "        loss = loss_fn(images, outputs)\n",
        "\n",
        "        batch_size = tf.shape(images)[0].numpy()\n",
        "        running_val_loss += loss.numpy() * batch_size\n",
        "        num_val_samples += batch_size\n",
        "\n",
        "    avg_val_loss = running_val_loss / num_val_samples\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}]  \"\n",
        "          f\"Train Loss: {avg_train_loss:.4f}  \"\n",
        "          f\"Val Loss:   {avg_val_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "100e7527-9a1c-4ddb-a454-78e19d0f3d1a",
      "metadata": {
        "id": "100e7527-9a1c-4ddb-a454-78e19d0f3d1a"
      },
      "source": [
        "Define SHA-12 & Loss Functions\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec20a969-94f2-45c1-9341-ce52b8c9cb71",
      "metadata": {
        "id": "ec20a969-94f2-45c1-9341-ce52b8c9cb71"
      },
      "outputs": [],
      "source": [
        "class DEC(tf.keras.Model):\n",
        "    def __init__(self, autoencoder, n_clusters=10, latent_dim=64):\n",
        "        super().__init__()\n",
        "        self.autoencoder = autoencoder\n",
        "        self.n_clusters = n_clusters\n",
        "        self.latent_dim = latent_dim\n",
        "        # Cluster centers: shape (n_clusters, latent_dim)\n",
        "        self.cluster_centers = tf.Variable(\n",
        "            tf.random.normal([n_clusters, latent_dim]), trainable=True, name=\"cluster_centers\"\n",
        "        )\n",
        "\n",
        "    def call(self, x):\n",
        "        _, z = self.autoencoder(x)  # z: (batch, latent_dim)\n",
        "        q = self.soft_assign(z)\n",
        "        return z, q\n",
        "\n",
        "    def soft_assign(self, z):\n",
        "        # z: (batch, latent_dim), cluster_centers: (n_clusters, latent_dim)\n",
        "        # Expand z to (batch, 1, latent_dim) and cluster_centers to (1, n_clusters, latent_dim)\n",
        "        expanded_z = tf.expand_dims(z, axis=1)\n",
        "        expanded_centers = tf.expand_dims(self.cluster_centers, axis=0)\n",
        "        dist = tf.reduce_sum(tf.square(expanded_z - expanded_centers), axis=2)  # (batch, n_clusters)\n",
        "        q = 1.0 / (1.0 + dist)\n",
        "        q = q ** ((1 + 1) / 2)\n",
        "        q = q / tf.reduce_sum(q, axis=1, keepdims=True)\n",
        "        return q\n",
        "\n",
        "def target_distribution(q):\n",
        "    weight = q ** 2 / tf.reduce_sum(q, axis=0)\n",
        "    # Transpose, normalize, then transpose back (for batch compatibility)\n",
        "    return tf.transpose(tf.transpose(weight) / tf.reduce_sum(weight, axis=1))\n",
        "\n",
        "def kl_divergence(p, q):\n",
        "    return tf.reduce_mean(tf.reduce_sum(p * tf.math.log((p + 1e-6) / (q + 1e-6)), axis=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "240e00b6-917a-426e-93ae-0634feacea6c",
      "metadata": {
        "id": "240e00b6-917a-426e-93ae-0634feacea6c"
      },
      "source": [
        "Initialize SHA-12 with KMeans\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccba4d97-d6ea-4be1-b2e1-241f9d960ef4",
      "metadata": {
        "id": "ccba4d97-d6ea-4be1-b2e1-241f9d960ef4"
      },
      "outputs": [],
      "source": [
        "# 1) Freeze decoder\n",
        "autoencoder.decoder.trainable = False\n",
        "\n",
        "# 2) Gather latents (with proper preprocessing!)\n",
        "latent_list = []\n",
        "for images, _ in train_ds:  # train_ds is batched\n",
        "    inputs = preprocess_for_resnet(images)\n",
        "    _, z = autoencoder(inputs, training=False)  # (batch, latent_dim)\n",
        "    latent_list.append(z.numpy())\n",
        "latent_features = np.concatenate(latent_list, axis=0)\n",
        "\n",
        "# KMeans init\n",
        "kmeans = KMeans(n_clusters=10, n_init=20, random_state=42)\n",
        "kmeans.fit(latent_features)\n",
        "\n",
        "# DEC model\n",
        "dec_model = DEC(autoencoder, n_clusters=10, latent_dim=latent_features.shape[1])\n",
        "\n",
        "# Set cluster centers to KMeans result\n",
        "dec_model.cluster_centers.assign(kmeans.cluster_centers_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "172a71f9-5f12-4161-8cef-331a5d5891ee",
      "metadata": {
        "id": "172a71f9-5f12-4161-8cef-331a5d5891ee"
      },
      "source": [
        "Fine-Tune Using Clustering Loss\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fd1f255-b375-44e0-bcac-e540271f10aa",
      "metadata": {
        "id": "1fd1f255-b375-44e0-bcac-e540271f10aa"
      },
      "outputs": [],
      "source": [
        "dec_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "num_dec_epochs = 10\n",
        "\n",
        "for epoch in range(num_dec_epochs):\n",
        "    # ——— Training Phase ———\n",
        "    train_kl = 0.0\n",
        "    num_train_samples = 0\n",
        "\n",
        "    for images, _ in train_ds:  # train_ds is batched\n",
        "        inputs = preprocess_for_resnet(images)\n",
        "        with tf.GradientTape() as tape:\n",
        "            z, q = dec_model(inputs, training=True)\n",
        "            p = target_distribution(tf.stop_gradient(q))\n",
        "            loss = kl_divergence(p, q)\n",
        "        grads = tape.gradient(loss, dec_model.trainable_variables)\n",
        "        dec_optimizer.apply_gradients(zip(grads, dec_model.trainable_variables))\n",
        "\n",
        "        batch_size = tf.shape(images)[0].numpy()\n",
        "        train_kl += loss.numpy() * batch_size\n",
        "        num_train_samples += batch_size\n",
        "\n",
        "    avg_train_kl = train_kl / num_train_samples\n",
        "\n",
        "    # ——— Validation Phase ———\n",
        "    val_kl = 0.0\n",
        "    num_val_samples = 0\n",
        "    for images, _ in test_ds:  # test_ds is batched\n",
        "        inputs = preprocess_for_resnet(images)\n",
        "        z, q = dec_model(inputs, training=False)\n",
        "        p = target_distribution(q)\n",
        "        loss = kl_divergence(p, q)\n",
        "        batch_size = tf.shape(images)[0].numpy()\n",
        "        val_kl += loss.numpy() * batch_size\n",
        "        num_val_samples += batch_size\n",
        "\n",
        "    avg_val_kl = val_kl / num_val_samples\n",
        "\n",
        "    print(f\"DEC Epoch [{epoch+1}/{num_dec_epochs}]  \"\n",
        "          f\"Train KL: {avg_train_kl:.6f}  \"\n",
        "          f\"Val KL:   {avg_val_kl:.6f}\")\n",
        "\n",
        "dec_val_kl = avg_val_kl"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d92f3215-e560-4370-88e8-984288858a4b",
      "metadata": {
        "id": "d92f3215-e560-4370-88e8-984288858a4b"
      },
      "source": [
        "Evaluate Clustering with Metrics\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47de377c-17df-4232-985f-9b48175d9c40",
      "metadata": {
        "id": "47de377c-17df-4232-985f-9b48175d9c40"
      },
      "outputs": [],
      "source": [
        "# Collect all latents and labels\n",
        "all_latents = []\n",
        "all_labels = []\n",
        "\n",
        "for images, labels in test_ds:  # test_ds is your batched tf.data.Dataset\n",
        "    inputs = preprocess_for_resnet(images)\n",
        "    _, z = dec_model.autoencoder(inputs, training=False)\n",
        "    all_latents.append(z.numpy())\n",
        "    all_labels.append(labels.numpy())\n",
        "\n",
        "all_latents = np.vstack(all_latents)\n",
        "all_labels = np.hstack(all_labels)\n",
        "\n",
        "# KMeans clustering\n",
        "kmeans_final = KMeans(n_clusters=10, random_state=42)\n",
        "cluster_labels = kmeans_final.fit_predict(all_latents)\n",
        "\n",
        "# Metrics\n",
        "sil_score = silhouette_score(all_latents, cluster_labels)\n",
        "db_score = davies_bouldin_score(all_latents, cluster_labels)\n",
        "ch_score = calinski_harabasz_score(all_latents, cluster_labels)\n",
        "\n",
        "print(f\"Silhouette Score:        {sil_score:.4f}\")\n",
        "print(f\"Davies-Bouldin Index:    {db_score:.4f}\")\n",
        "print(f\"Calinski-Harabasz Index: {ch_score:.4f}\")\n",
        "print(f\"KL: {dec_val_kl:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbfd7234-60d4-4c4a-9585-3097a3b3c23a",
      "metadata": {
        "id": "bbfd7234-60d4-4c4a-9585-3097a3b3c23a"
      },
      "source": [
        "Visualize Clusters with t-SNE\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac64455f-4ebc-4e7b-8dfa-e02ac54cd78f",
      "metadata": {
        "id": "ac64455f-4ebc-4e7b-8dfa-e02ac54cd78f"
      },
      "outputs": [],
      "source": [
        "# Assuming all_latents is (N, latent_dim) numpy array and cluster_labels is (N,) numpy array or list\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=500)\n",
        "latents_2d = tsne.fit_transform(all_latents)\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.scatterplot(x=latents_2d[:, 0], y=latents_2d[:, 1], hue=cluster_labels, palette='tab10', legend='full', s=15)\n",
        "plt.title(\"t-SNE Visualization \")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32588e1c-58e5-4e65-8e7e-6526902babdb",
      "metadata": {
        "id": "32588e1c-58e5-4e65-8e7e-6526902babdb"
      },
      "source": [
        "SOM Processing\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e852219-99aa-48cf-9463-2112c2956fac",
      "metadata": {
        "id": "9e852219-99aa-48cf-9463-2112c2956fac"
      },
      "outputs": [],
      "source": [
        "class SOM:\n",
        "    def __init__(self, map_size, input_dim, learning_rate=0.5, sigma=None):\n",
        "        self.map_size = map_size\n",
        "        self.input_dim = input_dim\n",
        "        self.learning_rate = learning_rate\n",
        "        self.sigma = sigma if sigma is not None else max(map_size) / 2\n",
        "\n",
        "        # Initialize weights randomly and normalize\n",
        "        weights = tf.random.normal((map_size[0], map_size[1], input_dim))\n",
        "        weights = weights / tf.norm(weights, axis=2, keepdims=True)\n",
        "        self.weights = tf.Variable(weights, trainable=False, dtype=tf.float32)\n",
        "\n",
        "        # Grid coordinates\n",
        "        x, y = tf.meshgrid(tf.range(map_size[0]), tf.range(map_size[1]), indexing='ij')\n",
        "        self.grid = tf.stack([x, y], axis=-1)\n",
        "        self.grid = tf.cast(self.grid, tf.float32)\n",
        "\n",
        "    def get_bmu(self, x):\n",
        "        # x: (batch_size, input_dim)\n",
        "        flat_weights = tf.reshape(self.weights, [-1, self.input_dim])  # (M*N, input_dim)\n",
        "        # Compute pairwise Euclidean distances: (batch_size, M*N)\n",
        "        dists = tf.norm(tf.expand_dims(x, 1) - flat_weights, axis=2)\n",
        "        bmu_indices = tf.argmin(dists, axis=1)  # (batch_size,)\n",
        "        bmu_coords = tf.stack([bmu_indices // self.map_size[1], bmu_indices % self.map_size[1]], axis=1)\n",
        "        return bmu_coords\n",
        "\n",
        "    def update(self, x, t, max_iter):\n",
        "        lr = self.learning_rate * np.exp(-t / max_iter)\n",
        "        sigma_t = self.sigma * np.exp(-t / max_iter)\n",
        "        bmu_coords = self.get_bmu(x)  # (batch_size, 2)\n",
        "\n",
        "        for i in range(x.shape[0]):\n",
        "            # Calculate distance to BMU on the grid\n",
        "            dist_to_bmu = tf.norm(self.grid - tf.cast(bmu_coords[i], tf.float32), axis=2)  # (map_x, map_y)\n",
        "            influence = tf.exp(-tf.square(dist_to_bmu) / (2 * sigma_t**2))  # (map_x, map_y)\n",
        "            diff = x[i] - self.weights  # (map_x, map_y, input_dim)\n",
        "            delta = lr * tf.expand_dims(influence, -1) * diff  # (map_x, map_y, input_dim)\n",
        "            self.weights.assign_add(delta)\n",
        "\n",
        "    def predict(self, x):\n",
        "        flat_weights = tf.reshape(self.weights, [-1, self.input_dim])  # (M*N, input_dim)\n",
        "        dists = tf.norm(tf.expand_dims(x, 1) - flat_weights, axis=2)  # (batch_size, M*N)\n",
        "        bmu_indices = tf.argmin(dists, axis=1)\n",
        "        return bmu_indices.numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbc4177e-f1a5-4386-805d-7ec4c080b99c",
      "metadata": {
        "id": "bbc4177e-f1a5-4386-805d-7ec4c080b99c"
      },
      "source": [
        "Extract Features\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8390d56c-1483-4134-9fa9-cfa77e175ac9",
      "metadata": {
        "id": "8390d56c-1483-4134-9fa9-cfa77e175ac9"
      },
      "outputs": [],
      "source": [
        "latent_features = []\n",
        "test_latents = []\n",
        "test_labels = []\n",
        "\n",
        "# Gather latent features from training data\n",
        "for images, _ in train_ds:\n",
        "    inputs = preprocess_for_resnet(images)\n",
        "    _, z = model(inputs, training=False)\n",
        "    latent_features.append(z.numpy())\n",
        "\n",
        "# Gather latent features and labels from test data\n",
        "for images, labels in test_ds:\n",
        "    inputs = preprocess_for_resnet(images)\n",
        "    _, z = model(inputs, training=False)\n",
        "    test_latents.append(z.numpy())\n",
        "    test_labels.append(labels.numpy())\n",
        "\n",
        "latent_features = np.vstack(latent_features)\n",
        "test_latents = np.vstack(test_latents)\n",
        "test_labels = np.hstack(test_labels)\n",
        "\n",
        "latent_dim = latent_features.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d8e2887-fe35-495d-80ab-f9c6ac896684",
      "metadata": {
        "id": "9d8e2887-fe35-495d-80ab-f9c6ac896684"
      },
      "outputs": [],
      "source": [
        "# Initial KMeans clustering (for DEC)\n",
        "kmeans_init = KMeans(n_clusters=10, n_init=20, random_state=42)\n",
        "initial_kmeans_labels = kmeans_init.fit_predict(latent_features)\n",
        "\n",
        "# Initial SOM clustering\n",
        "som = SOM(map_size=(10, 10), input_dim=latent_dim)\n",
        "initial_som_weights = som.weights.numpy().reshape(-1, latent_dim)\n",
        "kmeans_som_init = KMeans(n_clusters=10, random_state=42)\n",
        "initial_som_labels = kmeans_som_init.fit_predict(initial_som_weights)\n",
        "\n",
        "# Visualize initial clustering\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=500)\n",
        "latents_2d = tsne.fit_transform(latent_features[:1000])  # Subsample for visualization\n",
        "som_weights_2d = tsne.fit_transform(initial_som_weights)\n",
        "\n",
        "plt.figure(figsize=(14, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.scatterplot(x=latents_2d[:, 0], y=latents_2d[:, 1], hue=initial_kmeans_labels[:1000], palette='tab10', s=15)\n",
        "plt.title(\"Initial KMeans Clustering (DEC)\")\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.scatterplot(x=som_weights_2d[:, 0], y=som_weights_2d[:, 1], hue=initial_som_labels, palette='tab10', s=15)\n",
        "plt.title(\"Initial SOM Weights Clustering\")\n",
        "plt.savefig('initial SOM clustering.png')\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd5f7070-0eaf-48a9-a36f-9dfda9f1d4c0",
      "metadata": {
        "id": "cd5f7070-0eaf-48a9-a36f-9dfda9f1d4c0"
      },
      "source": [
        "Train SOM\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8520ad51-a5f1-4f25-85da-d71208592b08",
      "metadata": {
        "id": "8520ad51-a5f1-4f25-85da-d71208592b08"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "som = SOM(map_size=(2, 5), input_dim=latent_dim)\n",
        "max_iter = 1000\n",
        "\n",
        "latent_tensor = tf.convert_to_tensor(latent_features, dtype=tf.float32)\n",
        "\n",
        "for t in range(max_iter):\n",
        "    indices = np.random.permutation(latent_features.shape[0])[:128]\n",
        "    batch = tf.gather(latent_tensor, indices)\n",
        "    som.update(batch, t, max_iter)\n",
        "    if (t + 1) % 100 == 0:\n",
        "        print(f\"SOM Iteration [{t+1}/{max_iter}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca6c4576-b049-4605-9660-903f484a3634",
      "metadata": {
        "id": "ca6c4576-b049-4605-9660-903f484a3634"
      },
      "outputs": [],
      "source": [
        "latent_tensor = tf.convert_to_tensor(test_latents, dtype=tf.float32)\n",
        "\n",
        "# Compute distances between test latents and all SOM weights (flattened)\n",
        "flat_weights = tf.reshape(som.weights, [-1, som.input_dim])  # (num_nodes, latent_dim)\n",
        "dists = tf.norm(tf.expand_dims(latent_tensor, 1) - flat_weights, axis=2)  # (num_samples, num_nodes)\n",
        "bmu_dists = tf.reduce_min(dists, axis=1)  # (num_samples,)\n",
        "\n",
        "quant_error_som = tf.reduce_mean(bmu_dists).numpy()\n",
        "\n",
        "print(f\"SOM Quantization Error: {quant_error_som:.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9c75bde-d5b8-43f1-a523-d438327e7a70",
      "metadata": {
        "id": "b9c75bde-d5b8-43f1-a523-d438327e7a70"
      },
      "outputs": [],
      "source": [
        "def som_detailed_summary(som):\n",
        "    # Compute sizes\n",
        "    rows, cols = som.map_size\n",
        "    n_neurons = rows * cols\n",
        "    w_shape = som.weights.shape          # (rows, cols, input_dim)\n",
        "    g_shape = som.grid.shape             # (rows, cols, 2)\n",
        "    # Parameter counts\n",
        "    total_weights = np.prod(w_shape)\n",
        "    total_grid_elems = np.prod(g_shape)\n",
        "\n",
        "    print(\"Self-Organizing Map (SOM) Summary\")\n",
        "    print(\"=\"*40)\n",
        "    print(f\"Map size:        {som.map_size}   (total neurons: {n_neurons})\")\n",
        "    print(f\"Input dim:       {som.input_dim}\")\n",
        "    print(f\"Learning rate:   {som.learning_rate}\")\n",
        "    print(f\"Sigma (radius):  {som.sigma}\")\n",
        "    print()\n",
        "    print(\"Structure:\")\n",
        "    print(f\"  ├─ weights (Parameter)    shape={tuple(w_shape)}    params={total_weights}\")\n",
        "    print(f\"  └─ grid    (Buffer)       shape={tuple(g_shape)}    elems={total_grid_elems}\")\n",
        "    print()\n",
        "    print(\"Parameter counts:\")\n",
        "    print(f\"  Total params:           {total_weights + total_grid_elems}\")\n",
        "    print(f\"  Trainable params:       {total_weights}\")\n",
        "    print(f\"  Non-trainable params:   {total_grid_elems}\")\n",
        "\n",
        "# Example usage:\n",
        "som = SOM(map_size=(2, 5), input_dim=latent_dim, learning_rate=0.5)\n",
        "som_detailed_summary(som)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad483e95-f627-41c9-9816-aad5a0be4714",
      "metadata": {
        "id": "ad483e95-f627-41c9-9816-aad5a0be4714"
      },
      "source": [
        "Evaluate SOM\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ac0d5ce-2ce3-4f3c-a6b9-e4d1171a9b3e",
      "metadata": {
        "id": "7ac0d5ce-2ce3-4f3c-a6b9-e4d1171a9b3e"
      },
      "outputs": [],
      "source": [
        "# Predict cluster assignments for test latents\n",
        "som_labels = som.predict(tf.convert_to_tensor(test_latents, dtype=tf.float32))\n",
        "\n",
        "# Compute clustering metrics\n",
        "sil_score_som = silhouette_score(test_latents, som_labels)\n",
        "db_score_som = davies_bouldin_score(test_latents, som_labels)\n",
        "ch_score_som = calinski_harabasz_score(test_latents, som_labels)\n",
        "\n",
        "print(\"\\nSOM Clustering Metrics:\")\n",
        "print(f\"Silhouette Score:        {sil_score_som:.4f}\")\n",
        "print(f\"Davies-Bouldin Index:    {db_score_som:.4f}\")\n",
        "print(f\"Calinski-Harabasz Index: {ch_score_som:.4f}\")\n",
        "print(f\"{'Quantization Error:':<25}{quant_error_som:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e76d3041-ae9d-4bcd-b6a5-ab9f8949744b",
      "metadata": {
        "id": "e76d3041-ae9d-4bcd-b6a5-ab9f8949744b"
      },
      "source": [
        "Visualize SOM clustering\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d46f9fd4-a13b-4b85-95df-bbea963d854f",
      "metadata": {
        "id": "d46f9fd4-a13b-4b85-95df-bbea963d854f"
      },
      "outputs": [],
      "source": [
        "# Run t-SNE on test latents\n",
        "test_latents_2d = tsne.fit_transform(test_latents)\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.scatterplot(x=test_latents_2d[:, 0], y=test_latents_2d[:, 1], hue=som_labels, palette='tab10', s=15)\n",
        "plt.title(\"t-SNE Visualization of SOM Clusters\")\n",
        "plt.show()\n",
        "plt.savefig('som_clustering.png')\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50fe7093-c18f-4ca0-9f23-7df2eb3fbd92",
      "metadata": {
        "id": "50fe7093-c18f-4ca0-9f23-7df2eb3fbd92"
      },
      "source": [
        "Comparing metric results of SHA-12 and SOM\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "698f3d64-8cef-4f8c-b7c0-0de99b57b8f6",
      "metadata": {
        "id": "698f3d64-8cef-4f8c-b7c0-0de99b57b8f6"
      },
      "outputs": [],
      "source": [
        "print(\"\\nComparison Table:\")\n",
        "print(f\"{'Metric':<35} {'DEC':<10} {'SOM':<10}\")\n",
        "print(f\"{'Silhouette Score':<35} {sil_score:.4f}    {sil_score_som:.4f}\")\n",
        "print(f\"{'Davies-Bouldin Index':<35} {db_score:.4f}    {db_score_som:.4f}\")\n",
        "print(f\"{'Calinski-Harabasz Index':<35} {ch_score:.4f}    {ch_score_som:.4f}\")\n",
        "print(f\"{'Final KL / Quantization Error':<35} {dec_val_kl:.4f}    {quant_error_som:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
